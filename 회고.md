# LLM 문서 생성 프로젝트 회고

## 🎯 목표
- 회사 프로젝트 소스에 `README.md`, `llms.txt`를 생성하여 코드에 대해 상담
- `README.md`, `llms.txt`까지 생성해보는 것으로 목표 설정

## ✅ 성공 사례

### 1. Claude + MCP 활용
- Claude에 filesystem MCP 붙여서 해줌 → **너무 잘함**
- 결과 : [README_genByClaude.md](./README_genByClaude.md)

### 2. GPT4.1mini + 로컬 MCP 구현
- MCP 기능 구현해서 `@tool`로 제공
- GPT-4.1-mini로 tool 너무 잘 사용
- system prompt 주고 `README.md`, `llms.txt` 잘 생성
 - 결과 : [README_generatedByGpt4.md](./README_generatedByGpt4-1mini.md)


## 🔧 삽질 과정

### 1. MCP 시도
- 아 MCP를 나도 붙여서 써보자
- 마침 cursor도 그냥 요청하면 된다함

### 2. HTTP → stdio
- HTTP 시도 실패, stdio로 해야한다 함
- 근데 stdio로도 계속 실패
- filesystem code 주고 물어봐도 hang만 걸림
- **이때가 17일 14시 쯤**

## 💭 욕심 + 망함

### 1. 추가 기능 시도 (실패)
강의에서 배운거 적용하고 끝내려고 했음:
- GitHub에서 소스 긁어오는 기능 추가하고 (**못함**)
- 소스 Chroma DB에 저장하고 (**못함**)
- Self RAG로 평가해서 생성 (**못함**)

### 2. 로컬 모델 적용 시도
하지만 local 모델로 돌려서 실제로 적용하고 싶다는 욕심이 생김

#### Qwen2:7B 시도
- 아마도 메모리 부족으로 안돌아감

#### Microsoft/Phi-3-mini-4k-instruct
- `bind_tools`가 미구현
- LangChain의 ChatOpenAI 등 공식 LLM 클래스는 `bind_tools`를 지원하지만, 직접 만든 LocalChatModel은 `bind_tools`를 지원하지 않습니다
- Ollama를 사용하거나 `bind_tools`를 구현하라고 함

#### Ollama 3.2 시도
- Ollama로 띄우고 시도
- Tool description, prompt 영어로 변경
- 도구를 사용하긴 하는데, 실패도 하고 잘 못씀
- README 파일을 (내용없이) 생성만 함

#### 직접 도구 구현
- 모델이 도구를 쓰는걸 포기하고 내가 도구가 되기로 함
- 프로젝트 경로만 받고 코드를 다 정리해서 프롬프트에 전달함

##### Microsoft/Phi-3-mini-4k-instruct
- 코드 하나씩 summary를 invoke하고
- 그걸 모아서 프롬프트에 전달하고 README를 작성하려고 함
- **→ 맥북 리부팅**

##### Qwen/Qwen3-4B
```
RuntimeError: MPS backend out of memory 
(MPS allocated: 16.57 GB, other allocations: 2.62 GB, max allowed: 20.40 GB). 
Tried to allocate 1.30 GB on private pool. 
Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations 
(may cause system failure).
```

##### Qwen/Qwen3-1.6B
- README를 생성하긴 함
- 결과 : [README.md](./README.md)

## 📝 결론
- 프로젝트는 망했음
- 로컬 모델 적용 시 메모리 제약과 도구 지원 문제로 어려움
- 작은 모델(Qwen3-1.6B)로는 기본적인 문서 생성 가능
- 커서를 제대로 사용해본 경험이 참 좋았음